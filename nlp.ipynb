{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#停用詞\n",
    "stop_words=['\\n', 'or', 'are', 'they', 'i', 'some', 'by', '—', \n",
    "            'even', 'the', 'to', 'a', 'and', 'of', 'in', 'on', 'for', \n",
    "            'that', 'with', 'is', 'as', 'could', 'its', 'this', 'other',\n",
    "            'an', 'have', 'more', 'at','don’t', 'can', 'only', 'most']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in d:\\anaconda\\envs\\torch\\lib\\site-packages (from scikit-learn) (1.24.1)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\n",
      "  Downloading scipy-1.10.1-cp38-cp38-win_amd64.whl.metadata (58 kB)\n",
      "     ---------------------------------------- 0.0/59.0 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/59.0 kB ? eta -:--:--\n",
      "     ------------------- ------------------ 30.7/59.0 kB 435.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 59.0/59.0 kB 518.5 kB/s eta 0:00:00\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.3.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl (9.3 MB)\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/9.3 MB 1.3 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.2/9.3 MB 2.3 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.5/9.3 MB 3.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.0/9.3 MB 5.5 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.0/9.3 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.2/9.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.7/9.3 MB 11.1 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.1/9.3 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.0/9.3 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.1/9.3 MB 14.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.0/9.3 MB 14.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.0/9.3 MB 15.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.3/9.3 MB 14.8 MB/s eta 0:00:00\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 0.0/302.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 302.2/302.2 kB 18.2 MB/s eta 0:00:00\n",
      "Downloading scipy-1.10.1-cp38-cp38-win_amd64.whl (42.2 MB)\n",
      "   ---------------------------------------- 0.0/42.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.0/42.2 MB 20.7 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 2.0/42.2 MB 21.0 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 2.1/42.2 MB 19.3 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 2.6/42.2 MB 14.9 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 4.6/42.2 MB 19.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 5.3/42.2 MB 19.8 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 6.1/42.2 MB 19.6 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 6.9/42.2 MB 18.4 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 7.8/42.2 MB 17.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 8.7/42.2 MB 17.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 9.2/42.2 MB 16.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 9.5/42.2 MB 15.6 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 9.7/42.2 MB 14.2 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 11.3/42.2 MB 14.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 11.9/42.2 MB 13.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 12.7/42.2 MB 14.6 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 13.3/42.2 MB 13.9 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 13.9/42.2 MB 13.1 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 14.5/42.2 MB 12.8 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 15.2/42.2 MB 12.6 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 15.9/42.2 MB 12.6 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 16.5/42.2 MB 12.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 16.9/42.2 MB 12.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 17.3/42.2 MB 11.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 17.4/42.2 MB 11.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 17.4/42.2 MB 11.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 17.9/42.2 MB 10.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 18.4/42.2 MB 10.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 18.4/42.2 MB 10.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 18.6/42.2 MB 9.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 19.3/42.2 MB 9.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 19.6/42.2 MB 9.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 19.9/42.2 MB 10.4 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 20.3/42.2 MB 10.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 20.6/42.2 MB 9.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 20.9/42.2 MB 9.4 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 21.2/42.2 MB 9.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 21.6/42.2 MB 9.0 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 22.0/42.2 MB 8.8 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 22.3/42.2 MB 8.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 22.7/42.2 MB 8.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 23.1/42.2 MB 8.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 23.3/42.2 MB 8.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 23.7/42.2 MB 8.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 24.1/42.2 MB 8.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 24.4/42.2 MB 8.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 24.8/42.2 MB 7.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 25.3/42.2 MB 7.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 25.8/42.2 MB 7.5 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 26.3/42.2 MB 7.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 26.6/42.2 MB 7.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 27.1/42.2 MB 7.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 27.6/42.2 MB 7.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 28.1/42.2 MB 7.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 28.6/42.2 MB 7.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 29.2/42.2 MB 7.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 29.6/42.2 MB 7.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 30.1/42.2 MB 7.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 30.6/42.2 MB 7.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 30.9/42.2 MB 7.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 31.5/42.2 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 32.0/42.2 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 32.3/42.2 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 32.7/42.2 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 33.1/42.2 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 33.6/42.2 MB 7.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 33.9/42.2 MB 7.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 34.2/42.2 MB 7.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 34.6/42.2 MB 7.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 35.1/42.2 MB 7.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 35.6/42.2 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 36.0/42.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 36.3/42.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 36.7/42.2 MB 7.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 37.2/42.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 37.6/42.2 MB 7.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 37.9/42.2 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 38.3/42.2 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 38.7/42.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 39.1/42.2 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 39.5/42.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 39.9/42.2 MB 7.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 40.3/42.2 MB 7.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 40.7/42.2 MB 7.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 41.1/42.2 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.4/42.2 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.8/42.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  42.2/42.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  42.2/42.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  42.2/42.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 42.2/42.2 MB 7.1 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.2 scipy-1.10.1 threadpoolctl-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入相關套件\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 語料：最後一句為問題，其他為回答\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary： ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "# 將語料轉換為詞頻矩陣，計算各個字詞出現的次數。\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 生字表\n",
    "word = vectorizer.get_feature_names_out()\n",
    "print (\"Vocabulary：\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW=\n",
      " [[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# 查看四句話的 BOW\n",
    "print (\"BOW=\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF=\n",
      " [[0.     0.4388 0.542  0.4388 0.     0.     0.3587 0.     0.4388]\n",
      " [0.     0.2723 0.     0.2723 0.     0.8532 0.2226 0.     0.2723]\n",
      " [0.5528 0.     0.     0.     0.5528 0.     0.2885 0.5528 0.    ]\n",
      " [0.     0.4388 0.542  0.4388 0.     0.     0.3587 0.     0.4388]]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF 轉換\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(X)\n",
    "print (\"TF-IDF=\\n\", np.around(tfidf.toarray(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.1034849000930086\n",
      "  (0, 1)\t0.43830038447620107\n",
      "  (0, 0)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# 最後一句與其他句的相似度比較\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print (cosine_similarity(tfidf[-1], tfidf[:-1], dense_output=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 字詞前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入相關套件\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試文章段落\n",
    "text=\"Today is a great day. It is even better than yesterday.\" + \\\n",
    "     \" And yesterday was the best day ever.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today is a great day.',\n",
       " 'It is even better than yesterday.',\n",
       " 'And yesterday was the best day ever.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today is a great day.',\n",
       " 'It is even better than yesterday.',\n",
       " 'And yesterday was the best day ever.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分割字句\n",
    "nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today',\n",
       " 'is',\n",
       " 'a',\n",
       " 'great',\n",
       " 'day',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'even',\n",
       " 'better',\n",
       " 'than',\n",
       " 'yesterday',\n",
       " '.',\n",
       " 'And',\n",
       " 'yesterday',\n",
       " 'was',\n",
       " 'the',\n",
       " 'best',\n",
       " 'day',\n",
       " 'ever',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分詞\n",
    "nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my system keep crash hi crash yesterday, our crash daili'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#詞行還原\n",
    "# 字根詞形還原(Stemming)\n",
    "text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "' '.join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keep crashing his crashed yesterday, ours crash daily'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 依字典規則的詞形還原(Lemmatization)\n",
    "text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
    "lem = nltk.WordNetLemmatizer()\n",
    "' '.join([lem.lemmatize(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 停用詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "標點符號: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print('標點符號:', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試文章段落\n",
    "text=\"Today is a great day. It is even better than yesterday.\" + \\\n",
    "     \" And yesterday was the best day ever.\"\n",
    "\n",
    "#讀取停用詞\n",
    "stopword_list = set(nltk.corpus.stopwords.words('english') \n",
    "                    + list(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today great day It even better yesterday And yesterday best day ever'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 移除停用詞(Removing Stopwords)\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    if is_lower_case:\n",
    "        text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text, filtered_tokens\n",
    "\n",
    "filtered_text, filtered_tokens = remove_stopwords(text) \n",
    "filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

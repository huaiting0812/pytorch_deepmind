{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#停用詞\n",
    "stop_words=['\\n', 'or', 'are', 'they', 'i', 'some', 'by', '—', \n",
    "            'even', 'the', 'to', 'a', 'and', 'of', 'in', 'on', 'for', \n",
    "            'that', 'with', 'is', 'as', 'could', 'its', 'this', 'other',\n",
    "            'an', 'have', 'more', 'at','don’t', 'can', 'only', 'most']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in d:\\anaconda\\envs\\torch\\lib\\site-packages (from scikit-learn) (1.24.1)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\n",
      "  Downloading scipy-1.10.1-cp38-cp38-win_amd64.whl.metadata (58 kB)\n",
      "     ---------------------------------------- 0.0/59.0 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/59.0 kB ? eta -:--:--\n",
      "     ------------------- ------------------ 30.7/59.0 kB 435.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 59.0/59.0 kB 518.5 kB/s eta 0:00:00\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.3.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl (9.3 MB)\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/9.3 MB 1.3 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.2/9.3 MB 2.3 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.5/9.3 MB 3.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.0/9.3 MB 5.5 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.0/9.3 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.2/9.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.7/9.3 MB 11.1 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.1/9.3 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.0/9.3 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.1/9.3 MB 14.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.0/9.3 MB 14.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.0/9.3 MB 15.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.3/9.3 MB 14.8 MB/s eta 0:00:00\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 0.0/302.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 302.2/302.2 kB 18.2 MB/s eta 0:00:00\n",
      "Downloading scipy-1.10.1-cp38-cp38-win_amd64.whl (42.2 MB)\n",
      "   ---------------------------------------- 0.0/42.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.0/42.2 MB 20.7 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 2.0/42.2 MB 21.0 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 2.1/42.2 MB 19.3 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 2.6/42.2 MB 14.9 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 4.6/42.2 MB 19.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 5.3/42.2 MB 19.8 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 6.1/42.2 MB 19.6 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 6.9/42.2 MB 18.4 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 7.8/42.2 MB 17.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 8.7/42.2 MB 17.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 9.2/42.2 MB 16.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 9.5/42.2 MB 15.6 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 9.7/42.2 MB 14.2 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 11.3/42.2 MB 14.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 11.9/42.2 MB 13.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 12.7/42.2 MB 14.6 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 13.3/42.2 MB 13.9 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 13.9/42.2 MB 13.1 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 14.5/42.2 MB 12.8 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 15.2/42.2 MB 12.6 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 15.9/42.2 MB 12.6 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 16.5/42.2 MB 12.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 16.9/42.2 MB 12.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 17.3/42.2 MB 11.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 17.4/42.2 MB 11.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 17.4/42.2 MB 11.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 17.9/42.2 MB 10.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 18.4/42.2 MB 10.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 18.4/42.2 MB 10.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 18.6/42.2 MB 9.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 19.3/42.2 MB 9.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 19.6/42.2 MB 9.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 19.9/42.2 MB 10.4 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 20.3/42.2 MB 10.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 20.6/42.2 MB 9.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 20.9/42.2 MB 9.4 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 21.2/42.2 MB 9.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 21.6/42.2 MB 9.0 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 22.0/42.2 MB 8.8 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 22.3/42.2 MB 8.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 22.7/42.2 MB 8.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 23.1/42.2 MB 8.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 23.3/42.2 MB 8.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 23.7/42.2 MB 8.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 24.1/42.2 MB 8.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 24.4/42.2 MB 8.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 24.8/42.2 MB 7.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 25.3/42.2 MB 7.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 25.8/42.2 MB 7.5 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 26.3/42.2 MB 7.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 26.6/42.2 MB 7.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 27.1/42.2 MB 7.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 27.6/42.2 MB 7.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 28.1/42.2 MB 7.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 28.6/42.2 MB 7.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 29.2/42.2 MB 7.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 29.6/42.2 MB 7.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 30.1/42.2 MB 7.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 30.6/42.2 MB 7.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 30.9/42.2 MB 7.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 31.5/42.2 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 32.0/42.2 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 32.3/42.2 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 32.7/42.2 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 33.1/42.2 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 33.6/42.2 MB 7.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 33.9/42.2 MB 7.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 34.2/42.2 MB 7.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 34.6/42.2 MB 7.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 35.1/42.2 MB 7.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 35.6/42.2 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 36.0/42.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 36.3/42.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 36.7/42.2 MB 7.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 37.2/42.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 37.6/42.2 MB 7.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 37.9/42.2 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 38.3/42.2 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 38.7/42.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 39.1/42.2 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 39.5/42.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 39.9/42.2 MB 7.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 40.3/42.2 MB 7.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 40.7/42.2 MB 7.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 41.1/42.2 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.4/42.2 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.8/42.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  42.2/42.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  42.2/42.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  42.2/42.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 42.2/42.2 MB 7.1 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.2 scipy-1.10.1 threadpoolctl-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入相關套件\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 語料：最後一句為問題，其他為回答\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary： ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "# 將語料轉換為詞頻矩陣，計算各個字詞出現的次數。\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 生字表\n",
    "word = vectorizer.get_feature_names_out()\n",
    "print (\"Vocabulary：\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW=\n",
      " [[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# 查看四句話的 BOW\n",
    "print (\"BOW=\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF=\n",
      " [[0.     0.4388 0.542  0.4388 0.     0.     0.3587 0.     0.4388]\n",
      " [0.     0.2723 0.     0.2723 0.     0.8532 0.2226 0.     0.2723]\n",
      " [0.5528 0.     0.     0.     0.5528 0.     0.2885 0.5528 0.    ]\n",
      " [0.     0.4388 0.542  0.4388 0.     0.     0.3587 0.     0.4388]]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF 轉換\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(X)\n",
    "print (\"TF-IDF=\\n\", np.around(tfidf.toarray(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.1034849000930086\n",
      "  (0, 1)\t0.43830038447620107\n",
      "  (0, 0)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# 最後一句與其他句的相似度比較\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print (cosine_similarity(tfidf[-1], tfidf[:-1], dense_output=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 字詞前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入相關套件\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試文章段落\n",
    "text=\"Today is a great day. It is even better than yesterday.\" + \\\n",
    "     \" And yesterday was the best day ever.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today is a great day.',\n",
       " 'It is even better than yesterday.',\n",
       " 'And yesterday was the best day ever.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\88693\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today is a great day.',\n",
       " 'It is even better than yesterday.',\n",
       " 'And yesterday was the best day ever.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分割字句\n",
    "nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today',\n",
       " 'is',\n",
       " 'a',\n",
       " 'great',\n",
       " 'day',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'even',\n",
       " 'better',\n",
       " 'than',\n",
       " 'yesterday',\n",
       " '.',\n",
       " 'And',\n",
       " 'yesterday',\n",
       " 'was',\n",
       " 'the',\n",
       " 'best',\n",
       " 'day',\n",
       " 'ever',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分詞\n",
    "nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my system keep crash hi crash yesterday, our crash daili'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#詞行還原\n",
    "# 字根詞形還原(Stemming)\n",
    "text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "' '.join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keep crashing his crashed yesterday, ours crash daily'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 依字典規則的詞形還原(Lemmatization)\n",
    "text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
    "lem = nltk.WordNetLemmatizer()\n",
    "' '.join([lem.lemmatize(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 停用詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "標點符號: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print('標點符號:', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試文章段落\n",
    "text=\"Today is a great day. It is even better than yesterday.\" + \\\n",
    "     \" And yesterday was the best day ever.\"\n",
    "\n",
    "#讀取停用詞\n",
    "stopword_list = set(nltk.corpus.stopwords.words('english') \n",
    "                    + list(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today great day It even better yesterday And yesterday best day ever'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 移除停用詞(Removing Stopwords)\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    if is_lower_case:\n",
    "        text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text, filtered_tokens\n",
    "\n",
    "filtered_text, filtered_tokens = remove_stopwords(text) \n",
    "filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec\n",
    "* bow和TF_IDF都是計算文字出現在文件中的次數\n",
    "* 這裡詞向量有兩個方法\n",
    "1. CBOW-以上下文去預測單字\n",
    "2. Continuos Skip-gram Model-以單字去預測上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp38-cp38-win_amd64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in d:\\anaconda\\envs\\torch\\lib\\site-packages (from gensim) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in d:\\anaconda\\envs\\torch\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.16.0-cp38-cp38-win_amd64.whl.metadata (6.8 kB)\n",
      "Downloading gensim-4.3.2-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.0 MB 1.3 MB/s eta 0:00:19\n",
      "   ---------------------------------------- 0.1/24.0 MB 1.1 MB/s eta 0:00:23\n",
      "   ---------------------------------------- 0.2/24.0 MB 2.0 MB/s eta 0:00:13\n",
      "    --------------------------------------- 0.5/24.0 MB 2.7 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 1.0/24.0 MB 4.4 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.0/24.0 MB 7.2 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 3.1/24.0 MB 9.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.1/24.0 MB 9.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.1/24.0 MB 8.0 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 4.7/24.0 MB 9.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 6.8/24.0 MB 13.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 8.5/24.0 MB 15.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 9.4/24.0 MB 15.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 10.3/24.0 MB 17.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 11.3/24.0 MB 21.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.2/24.0 MB 21.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 13.1/24.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 14.0/24.0 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.9/24.0 MB 24.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 15.9/24.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 16.8/24.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 17.7/24.0 MB 19.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 18.6/24.0 MB 19.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 19.6/24.0 MB 19.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 20.9/24.0 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.1/24.0 MB 20.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.2/24.0 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 18.7 MB/s eta 0:00:00\n",
      "Downloading smart_open-7.0.1-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.8/60.8 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading wrapt-1.16.0-cp38-cp38-win_amd64.whl (37 kB)\n",
      "Installing collected packages: wrapt, smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-7.0.1 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入相關套件\n",
    "import pprint  # 較美觀的列印函數\n",
    "import gensim\n",
    "from collections import defaultdict\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 語料庫\n",
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
       " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'management', 'system'],\n",
       " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
       " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
       " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
       " ['intersection', 'graph', 'paths', 'trees'],\n",
       " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoplist = set('for a of the and to in'.split()) #停用詞\n",
    "\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents] #轉小寫並將停用詞刪除\n",
    "\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#單詞出現次數統計\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "for text in texts:\n",
    "\n",
    "    for token in text:\n",
    "\n",
    "        frequency[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'human': 2,\n",
       "             'machine': 1,\n",
       "             'interface': 2,\n",
       "             'lab': 1,\n",
       "             'abc': 1,\n",
       "             'computer': 2,\n",
       "             'applications': 1,\n",
       "             'survey': 2,\n",
       "             'user': 3,\n",
       "             'opinion': 1,\n",
       "             'system': 4,\n",
       "             'response': 2,\n",
       "             'time': 2,\n",
       "             'eps': 2,\n",
       "             'management': 1,\n",
       "             'engineering': 1,\n",
       "             'testing': 1,\n",
       "             'relation': 1,\n",
       "             'perceived': 1,\n",
       "             'error': 1,\n",
       "             'measurement': 1,\n",
       "             'generation': 1,\n",
       "             'random': 1,\n",
       "             'binary': 1,\n",
       "             'unordered': 1,\n",
       "             'trees': 3,\n",
       "             'intersection': 1,\n",
       "             'graph': 3,\n",
       "             'paths': 1,\n",
       "             'minors': 2,\n",
       "             'iv': 1,\n",
       "             'widths': 1,\n",
       "             'well': 1,\n",
       "             'quasi': 1,\n",
       "             'ordering': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    [token for token in text if frequency[token] > 1]\n",
    "    for text in texts\n",
    "]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
       " [(1, 1), (5, 2), (8, 1)],\n",
       " [(3, 1), (6, 1), (7, 1)],\n",
       " [(9, 1)],\n",
       " [(9, 1), (10, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(4, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 轉為字典\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# 轉為 BOW\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立LSI模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.644*\"system\" + 0.404*\"user\" + 0.301*\"eps\" + 0.265*\"response\" + 0.265*\"time\" + 0.240*\"computer\" + 0.221*\"human\" + 0.206*\"survey\" + 0.198*\"interface\" + 0.036*\"graph\"'),\n",
       " (1,\n",
       "  '0.623*\"graph\" + 0.490*\"trees\" + 0.451*\"minors\" + 0.274*\"survey\" + -0.167*\"system\" + -0.141*\"eps\" + -0.113*\"human\" + 0.107*\"response\" + 0.107*\"time\" + -0.072*\"interface\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 建立 LSI (Latent semantic indexing) 模型\n",
    "from gensim import models\n",
    "\n",
    "# num_topics=2：取二維，即兩個議題\n",
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "\n",
    "# 兩個議題的 LSI 公式\n",
    "lsi.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.46182100453271535), (1, -0.07002766527899937)]\n"
     ]
    }
   ],
   "source": [
    "# 例句\n",
    "doc = \"Human computer interaction\"\n",
    "\n",
    "# 測試 LSI (Latent semantic indexing) 模型\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow]  \n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.998093), (1, 0.93748635), (2, 0.9984453), (3, 0.9865886), (4, 0.90755945), (5, -0.12416792), (6, -0.10639259), (7, -0.09879464), (8, 0.050041765)]\n"
     ]
    }
   ],
   "source": [
    "# 比較例句與語料庫每一句的相似機率\n",
    "from gensim import similarities\n",
    "# 比較例句與語料庫的相似性索引\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])  \n",
    "# 比較例句與語料庫的相似機率\n",
    "sims = index[vec_lsi]  \n",
    "# 顯示語料庫的索引值及相似機率\n",
    "print(list(enumerate(sims)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9984453 The EPS user interface management system\n",
      "0.998093 Human machine interface for lab abc computer applications\n",
      "0.9865886 System and human system engineering testing of EPS\n",
      "0.93748635 A survey of user opinion of computer system response time\n",
      "0.90755945 Relation of user perceived response time to error measurement\n",
      "0.050041765 Graph minors A survey\n",
      "-0.09879464 Graph minors IV Widths of trees and well quasi ordering\n",
      "-0.10639259 The intersection graph of paths in trees\n",
      "-0.12416792 The generation of random binary unordered trees\n"
     ]
    }
   ],
   "source": [
    "# 依相似機率降冪排序\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "for doc_position, doc_score in sims:\n",
    "    print(doc_score, documents[doc_position])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
